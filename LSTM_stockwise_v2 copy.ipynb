{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Enhanced Robo Advisor: LSTM for Indian Stock Price Prediction\n",
    "\n",
    "## MTech Project - Financial Time Series Analysis (2015-2025)\n",
    "\n",
    "This notebook mirrors the existing Autoformer pipeline but replaces the model with an LSTM. It trains a separate model per stock (Nifty 50 subset), evaluates, and visualizes results.\n",
    "\n",
    "Table of Contents\n",
    "1. Environment Setup & Imports\n",
    "2. Data Download & Preparation\n",
    "3. Data Cleaning & Preprocessing\n",
    "4. Feature Engineering\n",
    "5. Stock-Specific Splits & Datasets\n",
    "6. LSTM Model Development\n",
    "7. Model Training (Per-Stock)\n",
    "8. Evaluation & Visualizations\n",
    "9. Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 00:16:28,161 - INFO - Configuration:\n",
      "2025-10-25 00:16:28,161 - INFO - tickers: ['RELIANCE.NS', 'TCS.NS', 'INFY.NS', 'HDFCBANK.NS', 'ICICIBANK.NS', 'HINDUNILVR.NS', 'ITC.NS', 'KOTAKBANK.NS', 'LT.NS', 'BHARTIARTL.NS']\n",
      "2025-10-25 00:16:28,162 - INFO - start_date: 2015-01-01\n",
      "2025-10-25 00:16:28,162 - INFO - end_date: 2025-01-01\n",
      "2025-10-25 00:16:28,163 - INFO - target_col: Close\n",
      "2025-10-25 00:16:28,163 - INFO - stock_specific_training: True\n",
      "2025-10-25 00:16:28,164 - INFO - train_years: 7\n",
      "2025-10-25 00:16:28,164 - INFO - val_years: 1.5\n",
      "2025-10-25 00:16:28,165 - INFO - test_years: 1.5\n",
      "2025-10-25 00:16:28,166 - INFO - input_dim: None\n",
      "2025-10-25 00:16:28,166 - INFO - hidden_dim: 256\n",
      "2025-10-25 00:16:28,166 - INFO - num_layers: 2\n",
      "2025-10-25 00:16:28,168 - INFO - dropout_rate: 0.2\n",
      "2025-10-25 00:16:28,168 - INFO - bidirectional: True\n",
      "2025-10-25 00:16:28,169 - INFO - attention: True\n",
      "2025-10-25 00:16:28,169 - INFO - sequence_length: 180\n",
      "2025-10-25 00:16:28,169 - INFO - forecast_horizon: 5\n",
      "2025-10-25 00:16:28,170 - INFO - batch_size: 128\n",
      "2025-10-25 00:16:28,170 - INFO - epochs: 100\n",
      "2025-10-25 00:16:28,170 - INFO - learning_rate: 0.0001\n",
      "2025-10-25 00:16:28,171 - INFO - weight_decay: 1e-05\n",
      "2025-10-25 00:16:28,171 - INFO - patience: 20\n",
      "2025-10-25 00:16:28,172 - INFO - lr_scheduler: cosine\n",
      "2025-10-25 00:16:28,172 - INFO - warmup_epochs: 5\n",
      "2025-10-25 00:16:28,173 - INFO - min_lr: 1e-06\n",
      "2025-10-25 00:16:28,173 - INFO - mse_weight: 0.6\n",
      "2025-10-25 00:16:28,173 - INFO - directional_weight: 0.25\n",
      "2025-10-25 00:16:28,174 - INFO - smoothness_weight: 0.15\n",
      "2025-10-25 00:16:28,174 - INFO - data_dir: data\n",
      "2025-10-25 00:16:28,174 - INFO - model_dir: models\n",
      "2025-10-25 00:16:28,174 - INFO - results_dir: results\n",
      "2025-10-25 00:16:28,175 - INFO - logs_dir: logs\n",
      "2025-10-25 00:16:28,175 - INFO - device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from typing import List, Dict, Tuple\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_name in ['data', 'models', 'results', 'logs']:\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Stock Selection\n",
    "    'tickers': [\n",
    "        'RELIANCE.NS', 'TCS.NS', 'INFY.NS', 'HDFCBANK.NS', 'ICICIBANK.NS',\n",
    "        'HINDUNILVR.NS', 'ITC.NS', 'KOTAKBANK.NS', 'LT.NS', 'BHARTIARTL.NS'\n",
    "    ],\n",
    "    'start_date': '2015-01-01',\n",
    "    'end_date': '2025-01-01',\n",
    "    'target_col': 'Close',\n",
    "\n",
    "    # Stock-Specific Training\n",
    "    'stock_specific_training': True,\n",
    "    'train_years': 7,\n",
    "    'val_years': 1.5,\n",
    "    'test_years': 1.5,\n",
    "\n",
    "    # Model Architecture\n",
    "    'input_dim': None,  # Will be set based on features\n",
    "    'hidden_dim': 256,  # Increased from 128\n",
    "    'num_layers': 2,\n",
    "    'dropout_rate': 0.2,\n",
    "    'bidirectional': True,\n",
    "    'attention': True,\n",
    "\n",
    "    # Training Parameters\n",
    "    'sequence_length': 180,  # Increased from 120\n",
    "    'forecast_horizon': 5,\n",
    "    'batch_size': 128,      # Increased from 64\n",
    "    'epochs': 100,          # Increased from 60\n",
    "    'learning_rate': 1e-4,  # Decreased from 5e-4\n",
    "    'weight_decay': 1e-5,\n",
    "    'patience': 20,         # Increased from 15\n",
    "    \n",
    "    # Learning Rate Schedule\n",
    "    'lr_scheduler': 'cosine',  # Options: 'cosine', 'reduce_on_plateau'\n",
    "    'warmup_epochs': 5,\n",
    "    'min_lr': 1e-6,\n",
    "    \n",
    "    # Loss Function Weights\n",
    "    'mse_weight': 0.6,\n",
    "    'directional_weight': 0.25,\n",
    "    'smoothness_weight': 0.15,\n",
    "\n",
    "    # Paths\n",
    "    'data_dir': 'data',\n",
    "    'model_dir': 'models',\n",
    "    'results_dir': 'results',\n",
    "    'logs_dir': 'logs',\n",
    "\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Print configuration\n",
    "logger.info(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    logger.info(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting data download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe3390bc24d4cbea7eb988af68e1712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading stock data:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 00:16:28,193 - INFO - Downloading data for RELIANCE.NS from 2015-01-01 to 2025-01-01 (Attempt 1)\n",
      "2025-10-25 00:16:28,979 - INFO - Successfully downloaded 2467 records for RELIANCE.NS\n",
      "2025-10-25 00:16:29,081 - INFO - Downloading data for TCS.NS from 2015-01-01 to 2025-01-01 (Attempt 1)\n",
      "2025-10-25 00:16:29,373 - INFO - Successfully downloaded 2467 records for TCS.NS\n",
      "2025-10-25 00:16:29,475 - INFO - Downloading data for INFY.NS from 2015-01-01 to 2025-01-01 (Attempt 1)\n",
      "2025-10-25 00:16:29,892 - INFO - Successfully downloaded 2467 records for INFY.NS\n",
      "2025-10-25 00:16:29,994 - INFO - Downloading data for HDFCBANK.NS from 2015-01-01 to 2025-01-01 (Attempt 1)\n",
      "2025-10-25 00:16:30,246 - INFO - Successfully downloaded 2467 records for HDFCBANK.NS\n",
      "2025-10-25 00:16:30,349 - INFO - Downloading data for ICICIBANK.NS from 2015-01-01 to 2025-01-01 (Attempt 1)\n",
      "2025-10-25 00:16:30,663 - INFO - Successfully downloaded 2467 records for ICICIBANK.NS\n",
      "2025-10-25 00:16:30,766 - INFO - Downloading data for HINDUNILVR.NS from 2015-01-01 to 2025-01-01 (Attempt 1)\n",
      "2025-10-25 00:16:31,011 - INFO - Successfully downloaded 2467 records for HINDUNILVR.NS\n",
      "2025-10-25 00:16:31,113 - INFO - Downloading data for ITC.NS from 2015-01-01 to 2025-01-01 (Attempt 1)\n",
      "2025-10-25 00:16:31,410 - INFO - Successfully downloaded 2467 records for ITC.NS\n",
      "2025-10-25 00:16:31,512 - INFO - Downloading data for KOTAKBANK.NS from 2015-01-01 to 2025-01-01 (Attempt 1)\n",
      "2025-10-25 00:16:31,819 - INFO - Successfully downloaded 2467 records for KOTAKBANK.NS\n",
      "2025-10-25 00:16:31,920 - INFO - Downloading data for LT.NS from 2015-01-01 to 2025-01-01 (Attempt 1)\n",
      "2025-10-25 00:16:32,153 - INFO - Successfully downloaded 2467 records for LT.NS\n",
      "2025-10-25 00:16:32,255 - INFO - Downloading data for BHARTIARTL.NS from 2015-01-01 to 2025-01-01 (Attempt 1)\n",
      "2025-10-25 00:16:32,538 - INFO - Successfully downloaded 2467 records for BHARTIARTL.NS\n",
      "2025-10-25 00:16:32,643 - INFO - Downloaded data shape: (24670, 13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data downloaded successfully\n",
      "Shape: (24670, 13)\n",
      "\n",
      "Sample of downloaded data:\n",
      "                       Date        Open        High         Low       Close  \\\n",
      "0 2015-01-01 00:00:00+05:30  189.657431  190.877155  189.090351  189.999802   \n",
      "1 2015-01-02 00:00:00+05:30  190.042601  191.743813  189.229456  189.496933   \n",
      "2 2015-01-05 00:00:00+05:30  189.379224  190.641759  187.046752  187.421234   \n",
      "3 2015-01-06 00:00:00+05:30  186.169449  186.811408  178.037899  178.915253   \n",
      "4 2015-01-07 00:00:00+05:30  179.129243  183.772776  179.107845  182.809830   \n",
      "\n",
      "     Volume  Dividends  Stock Splits       Ticker   Returns  Log_Returns  \\\n",
      "0   2963643        0.0           0.0  RELIANCE.NS       NaN          NaN   \n",
      "1   7331366        0.0           0.0  RELIANCE.NS -0.002647    -0.002650   \n",
      "2  10103941        0.0           0.0  RELIANCE.NS -0.010954    -0.011014   \n",
      "3  18627980        0.0           0.0  RELIANCE.NS -0.045384    -0.046446   \n",
      "4  20720312        0.0           0.0  RELIANCE.NS  0.021768     0.021534   \n",
      "\n",
      "   Volatility   Adj Close  \n",
      "0         NaN  189.999802  \n",
      "1         NaN  189.496933  \n",
      "2         NaN  187.421234  \n",
      "3         NaN  178.915253  \n",
      "4         NaN  182.809830  \n",
      "\n",
      "Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24670 entries, 0 to 24669\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count  Dtype                       \n",
      "---  ------        --------------  -----                       \n",
      " 0   Date          24670 non-null  datetime64[ns, Asia/Kolkata]\n",
      " 1   Open          24670 non-null  float64                     \n",
      " 2   High          24670 non-null  float64                     \n",
      " 3   Low           24670 non-null  float64                     \n",
      " 4   Close         24670 non-null  float64                     \n",
      " 5   Volume        24670 non-null  int64                       \n",
      " 6   Dividends     24670 non-null  float64                     \n",
      " 7   Stock Splits  24670 non-null  float64                     \n",
      " 8   Ticker        24670 non-null  object                      \n",
      " 9   Returns       24660 non-null  float64                     \n",
      " 10  Log_Returns   24660 non-null  float64                     \n",
      " 11  Volatility    24470 non-null  float64                     \n",
      " 12  Adj Close     24670 non-null  float64                     \n",
      "dtypes: datetime64[ns, Asia/Kolkata](1), float64(10), int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n",
      "None\n",
      "\n",
      "Data saved to data/raw_stock_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Data Download & Preparation\n",
    "def download_stock_data(ticker: str, start_date: str, end_date: str, max_retries: int = 3) -> pd.DataFrame:\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logger.info(f\"Downloading data for {ticker} from {start_date} to {end_date} (Attempt {attempt + 1})\")\n",
    "            data = yf.Ticker(ticker).history(start=start_date, end=end_date)\n",
    "            \n",
    "            if data.empty:\n",
    "                logger.warning(f\"No data retrieved for {ticker}\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "            data.reset_index(inplace=True)\n",
    "            data['Ticker'] = ticker\n",
    "            data['Date'] = pd.to_datetime(data['Date'])\n",
    "            \n",
    "            # Calculate basic features\n",
    "            data['Returns'] = data['Close'].pct_change()\n",
    "            data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
    "            data['Volatility'] = data['Returns'].rolling(window=20).std()\n",
    "            \n",
    "            # Ensure all required columns exist\n",
    "            if 'Adj Close' not in data.columns:\n",
    "                data['Adj Close'] = data['Close']\n",
    "                \n",
    "            # Basic data validation\n",
    "            required_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "            if any(col not in data.columns for col in required_cols):\n",
    "                missing = [col for col in required_cols if col not in data.columns]\n",
    "                logger.error(f\"Missing required columns for {ticker}: {missing}\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "            # Check for sufficient data points\n",
    "            if len(data) < 50:  # Minimum required data points\n",
    "                logger.warning(f\"Insufficient data points for {ticker}: {len(data)}\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "            logger.info(f\"Successfully downloaded {len(data)} records for {ticker}\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading {ticker} (Attempt {attempt + 1}): {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                logger.error(f\"Failed to download {ticker} after {max_retries} attempts\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "def download_all_stocks(tickers: List[str], start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    failed_tickers = []\n",
    "    \n",
    "    for t in tqdm(tickers, desc=\"Downloading stock data\", disable=None):\n",
    "        df = download_stock_data(t, start_date, end_date)\n",
    "        if not df.empty:\n",
    "            frames.append(df)\n",
    "        else:\n",
    "            failed_tickers.append(t)\n",
    "        time.sleep(0.1)  # Rate limiting\n",
    "        \n",
    "    if failed_tickers:\n",
    "        logger.warning(f\"Failed to download data for: {', '.join(failed_tickers)}\")\n",
    "        \n",
    "    if not frames:\n",
    "        logger.error(\"No data downloaded for any ticker\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    result = pd.concat(frames, ignore_index=True)\n",
    "    logger.info(f\"Downloaded data shape: {result.shape}\")\n",
    "    return result\n",
    "\n",
    "print(\"🚀 Starting data download...\")\n",
    "raw_data = download_all_stocks(CONFIG['tickers'], CONFIG['start_date'], CONFIG['end_date'])\n",
    "\n",
    "if not raw_data.empty:\n",
    "    print(f\"✅ Data downloaded successfully\")\n",
    "    print(f\"Shape: {raw_data.shape}\")\n",
    "    print(\"\\nSample of downloaded data:\")\n",
    "    print(raw_data.head())\n",
    "    print(\"\\nData info:\")\n",
    "    print(raw_data.info())\n",
    "    \n",
    "    # Save raw data\n",
    "    raw_data.to_csv(f\"{CONFIG['data_dir']}/raw_stock_data.csv\", index=False)\n",
    "    print(f\"\\nData saved to {CONFIG['data_dir']}/raw_stock_data.csv\")\n",
    "else:\n",
    "    print(\"❌ Data download failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Layer\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        attention_weights = self.attention(hidden_states)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        attended = torch.sum(attention_weights * hidden_states, dim=1)\n",
    "        return attended, attention_weights\n",
    "\n",
    "# Improved LSTM Model\n",
    "class ImprovedLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, num_layers=2, dropout=0.2, \n",
    "                 bidirectional=True, attention=True, out_len=5):\n",
    "        super().__init__()\n",
    "        self.out_len = out_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.attention = attention\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        if attention:\n",
    "            self.attention_layer = AttentionLayer(hidden_dim * (2 if bidirectional else 1))\n",
    "        \n",
    "        # Decoder - hidden size must match encoder's output dimension\n",
    "        decoder_hidden_size = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=1,  # Previous prediction\n",
    "            hidden_size=decoder_hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(decoder_hidden_size)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(decoder_hidden_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        batch_size = x.size(0)\n",
    "        enc_out, (h, c) = self.encoder(x)\n",
    "        \n",
    "        # Apply attention if enabled\n",
    "        if self.attention:\n",
    "            context, _ = self.attention_layer(enc_out)\n",
    "            context = context.unsqueeze(1)\n",
    "        else:\n",
    "            context = enc_out[:, -1, :].unsqueeze(1)  # Take last output as context\n",
    "        \n",
    "        # Initialize decoder input\n",
    "        dec_input = torch.zeros(batch_size, 1, 1, device=x.device)\n",
    "        \n",
    "        # Initialize decoder states - properly reshape from bidirectional encoder\n",
    "        if self.bidirectional:\n",
    "            # For bidirectional encoder: h and c have shape (num_layers*2, batch, hidden_dim)\n",
    "            # Reshape to (num_layers, batch, hidden_dim*2)\n",
    "            h = h.view(self.num_layers, 2, batch_size, self.hidden_dim)\n",
    "            h = torch.cat([h[:, 0], h[:, 1]], dim=2)  # Concatenate forward and backward\n",
    "            c = c.view(self.num_layers, 2, batch_size, self.hidden_dim)\n",
    "            c = torch.cat([c[:, 0], c[:, 1]], dim=2)  # Concatenate forward and backward\n",
    "        \n",
    "        # Initialize decoder states\n",
    "        dec_h = h\n",
    "        dec_c = c\n",
    "        \n",
    "        outputs = []\n",
    "        for _ in range(self.out_len):\n",
    "            # Decoder step\n",
    "            dec_out, (dec_h, dec_c) = self.decoder(dec_input, (dec_h, dec_c))\n",
    "            \n",
    "            # Combine with context\n",
    "            combined = dec_out + context\n",
    "            \n",
    "            # Apply normalization and dropout\n",
    "            combined = self.layer_norm(combined.squeeze(1))\n",
    "            combined = self.dropout(combined)\n",
    "            \n",
    "            # Generate prediction\n",
    "            pred = self.proj(combined).unsqueeze(1)\n",
    "            outputs.append(pred)\n",
    "            \n",
    "            # Update decoder input\n",
    "            dec_input = pred.detach()\n",
    "            \n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# Enhanced Loss Function\n",
    "class EnhancedLoss(nn.Module):\n",
    "    def __init__(self, mse_weight=0.6, directional_weight=0.25, smoothness_weight=0.15):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.mse_weight = mse_weight\n",
    "        self.directional_weight = directional_weight\n",
    "        self.smoothness_weight = smoothness_weight\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # MSE Loss\n",
    "        mse_loss = self.mse(pred, target)\n",
    "        \n",
    "        # Directional Loss\n",
    "        if pred.shape[1] > 1:\n",
    "            pred_diff = pred[:, 1:] - pred[:, :-1]\n",
    "            target_diff = target[:, 1:] - target[:, :-1]\n",
    "            directional_loss = torch.mean((torch.sign(pred_diff) != torch.sign(target_diff)).float())\n",
    "            \n",
    "            # Smoothness Loss (penalize large jumps in predictions)\n",
    "            smoothness_loss = torch.mean(torch.abs(pred_diff))\n",
    "        else:\n",
    "            directional_loss = pred.new_tensor(0.0)\n",
    "            smoothness_loss = pred.new_tensor(0.0)\n",
    "            \n",
    "        # Combine losses\n",
    "        total_loss = (self.mse_weight * mse_loss + \n",
    "                     self.directional_weight * directional_loss + \n",
    "                     self.smoothness_weight * smoothness_loss)\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting data download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026a691fccd041dab17af6a97d979bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading stock data:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 00:16:32,908 - INFO - Downloading data for RELIANCE.NS from 2015-01-01 to 2025-01-01\n",
      "2025-10-25 00:16:33,033 - INFO - Downloading data for TCS.NS from 2015-01-01 to 2025-01-01\n",
      "2025-10-25 00:16:33,155 - INFO - Downloading data for INFY.NS from 2015-01-01 to 2025-01-01\n",
      "2025-10-25 00:16:33,278 - INFO - Downloading data for HDFCBANK.NS from 2015-01-01 to 2025-01-01\n",
      "2025-10-25 00:16:33,399 - INFO - Downloading data for ICICIBANK.NS from 2015-01-01 to 2025-01-01\n",
      "2025-10-25 00:16:33,522 - INFO - Downloading data for HINDUNILVR.NS from 2015-01-01 to 2025-01-01\n",
      "2025-10-25 00:16:33,640 - INFO - Downloading data for ITC.NS from 2015-01-01 to 2025-01-01\n",
      "2025-10-25 00:16:33,761 - INFO - Downloading data for KOTAKBANK.NS from 2015-01-01 to 2025-01-01\n",
      "2025-10-25 00:16:33,881 - INFO - Downloading data for LT.NS from 2015-01-01 to 2025-01-01\n",
      "2025-10-25 00:16:34,001 - INFO - Downloading data for BHARTIARTL.NS from 2015-01-01 to 2025-01-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data downloaded (24670, 13)\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Download & Preparation\n",
    "\n",
    "def download_stock_data(ticker: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        logger.info(f\"Downloading data for {ticker} from {start_date} to {end_date}\")\n",
    "        data = yf.Ticker(ticker).history(start=start_date, end=end_date)\n",
    "        if data.empty:\n",
    "            logger.warning(f\"No data for {ticker}\")\n",
    "            return pd.DataFrame()\n",
    "        data.reset_index(inplace=True)\n",
    "        data['Ticker'] = ticker\n",
    "        data['Date'] = pd.to_datetime(data['Date'])\n",
    "        data['Returns'] = data['Close'].pct_change()\n",
    "        data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
    "        data['Volatility'] = data['Returns'].rolling(window=20).std()\n",
    "        if 'Adj Close' not in data.columns:\n",
    "            data['Adj Close'] = data['Close']\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading {ticker}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def download_all_stocks(tickers: List[str], start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for t in tqdm(tickers, desc=\"Downloading stock data\"):\n",
    "        df = download_stock_data(t, start_date, end_date)\n",
    "        if not df.empty:\n",
    "            frames.append(df)\n",
    "        time.sleep(0.1)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "print(\"🚀 Starting data download...\")\n",
    "raw_data = download_all_stocks(CONFIG['tickers'], CONFIG['start_date'], CONFIG['end_date'])\n",
    "if not raw_data.empty:\n",
    "    print(\"✅ Data downloaded\", raw_data.shape)\n",
    "    raw_data.to_csv(f\"{CONFIG['data_dir']}/raw_stock_data.csv\", index=False)\n",
    "else:\n",
    "    print(\"❌ Data download failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 00:16:34,375 - INFO - 🧹 Cleaning data...\n",
      "2025-10-25 00:16:34,410 - INFO - ✅ Cleaned shape: (24670, 20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned and saved (24670, 20)\n"
     ]
    }
   ],
   "source": [
    "# 3. Data Cleaning & Preprocessing\n",
    "\n",
    "def clean_stock_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger.info(\"🧹 Cleaning data...\")\n",
    "    c = df.copy()\n",
    "    c = c.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "    if 'Adj Close' not in c.columns and 'Close' in c.columns:\n",
    "        c['Adj Close'] = c['Close']\n",
    "    required = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    if any(col not in c.columns for col in required):\n",
    "        logger.error(\"Missing required columns\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Drop duplicates\n",
    "    c = c.drop_duplicates(subset=['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "    # Forward/backward fill per ticker\n",
    "    filled = []\n",
    "    for t in c['Ticker'].unique():\n",
    "        td = c[c['Ticker'] == t].copy()\n",
    "        for col in ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']:\n",
    "            if col in td.columns:\n",
    "                td[col] = td[col].ffill().bfill()\n",
    "        filled.append(td)\n",
    "    c = pd.concat(filled, ignore_index=True)\n",
    "\n",
    "    # Remove rows with missing criticals\n",
    "    c = c.dropna(subset=['Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "\n",
    "    # Add calendar features\n",
    "    c['Year'] = c['Date'].dt.year\n",
    "    c['Month'] = c['Date'].dt.month\n",
    "    c['Day'] = c['Date'].dt.day\n",
    "    c['DayOfWeek'] = c['Date'].dt.dayofweek\n",
    "    c['Quarter'] = c['Date'].dt.quarter\n",
    "    c['IsMonthEnd'] = c['Date'].dt.is_month_end\n",
    "    c['IsQuarterEnd'] = c['Date'].dt.is_quarter_end\n",
    "\n",
    "    logger.info(f\"✅ Cleaned shape: {c.shape}\")\n",
    "    return c\n",
    "\n",
    "if not raw_data.empty:\n",
    "    cleaned_data = clean_stock_data(raw_data)\n",
    "    if not cleaned_data.empty:\n",
    "        cleaned_data.to_csv(f\"{CONFIG['data_dir']}/cleaned_stock_data.csv\", index=False)\n",
    "        print(\"✅ Cleaned and saved\", cleaned_data.shape)\n",
    "    else:\n",
    "        print(\"❌ Cleaning failed\")\n",
    "else:\n",
    "    print(\"❌ No raw data to clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327233c1be934cd4b0968a1003403671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Engineering features:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BHARTIARTL.NS...\n",
      "Processing HDFCBANK.NS...\n",
      "Processing HINDUNILVR.NS...\n",
      "Processing ICICIBANK.NS...\n",
      "Processing INFY.NS...\n",
      "Processing ITC.NS...\n",
      "Processing KOTAKBANK.NS...\n",
      "Processing LT.NS...\n",
      "Processing RELIANCE.NS...\n",
      "Processing TCS.NS...\n",
      "Combining features...\n",
      "Adding market features...\n",
      "✅ Engineered and saved (24660, 79)\n",
      "\n",
      "Top 10 features by correlation with target:\n",
      "Close        1.000000\n",
      "Adj Close    1.000000\n",
      "High         0.999905\n",
      "Low          0.999895\n",
      "EMA_5        0.999779\n",
      "Open         0.999771\n",
      "SMA_5        0.999665\n",
      "EMA_10       0.999459\n",
      "EMA_12       0.999335\n",
      "SMA_10       0.999225\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 4. Feature Engineering\n",
    "\n",
    "def calculate_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    r = df.copy()\n",
    "    \n",
    "    # Moving Averages and Trends\n",
    "    windows = [5, 10, 20, 50, 100]\n",
    "    for w in windows:\n",
    "        r[f'SMA_{w}'] = r['Close'].rolling(w, min_periods=1).mean()\n",
    "        r[f'EMA_{w}'] = r['Close'].ewm(span=w, adjust=False).mean()\n",
    "    \n",
    "    # Ensure EMA_12 and EMA_26 exist for MACD\n",
    "    if 'EMA_12' not in r.columns:\n",
    "        r['EMA_12'] = r['Close'].ewm(span=12, adjust=False).mean()\n",
    "    if 'EMA_26' not in r.columns:\n",
    "        r['EMA_26'] = r['Close'].ewm(span=26, adjust=False).mean()\n",
    "        \n",
    "    # MACD with different parameters\n",
    "    r['MACD'] = r['EMA_12'] - r['EMA_26']\n",
    "    r['MACD_Signal'] = r['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    r['MACD_Hist'] = r['MACD'] - r['MACD_Signal']\n",
    "    \n",
    "    # Bollinger Bands with multiple standard deviations\n",
    "    for std in [1.5, 2, 2.5]:\n",
    "        r[f'BB_Middle_{std}'] = r['Close'].rolling(20, min_periods=1).mean()\n",
    "        bb_std = r['Close'].rolling(20, min_periods=1).std()\n",
    "        r[f'BB_Upper_{std}'] = r[f'BB_Middle_{std}'] + std * bb_std\n",
    "        r[f'BB_Lower_{std}'] = r[f'BB_Middle_{std}'] - std * bb_std\n",
    "        r[f'BB_Width_{std}'] = (r[f'BB_Upper_{std}'] - r[f'BB_Lower_{std}']) / r[f'BB_Middle_{std}']\n",
    "\n",
    "    # RSI calculation\n",
    "    def rsi(prices, window=14):\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    for w in [6, 14, 28]:\n",
    "        r[f'RSI_{w}'] = rsi(r['Close'], w)\n",
    "\n",
    "    # Stochastic Oscillator\n",
    "    for k_period in [14, 21]:\n",
    "        for d_period in [3, 5]:\n",
    "            lowest_low = r['Low'].rolling(k_period, min_periods=1).min()\n",
    "            highest_high = r['High'].rolling(k_period, min_periods=1).max()\n",
    "            r[f'Stoch_K_{k_period}'] = 100 * (r['Close'] - lowest_low) / (highest_high - lowest_low + 1e-8)\n",
    "            r[f'Stoch_D_{k_period}_{d_period}'] = r[f'Stoch_K_{k_period}'].rolling(d_period, min_periods=1).mean()\n",
    "\n",
    "    # Momentum Indicators\n",
    "    for period in [10, 21, 63]:\n",
    "        r[f'Momentum_{period}'] = r['Close'] / r['Close'].shift(period) - 1\n",
    "        r[f'ROC_{period}'] = (r['Close'] - r['Close'].shift(period)) / r['Close'].shift(period) * 100\n",
    "\n",
    "    # Volatility Indicators\n",
    "    r['ATR_14'] = pd.Series(np.maximum(\n",
    "        r['High'] - r['Low'],\n",
    "        np.maximum(\n",
    "            (r['High'] - r['Close'].shift(1)).abs(),\n",
    "            (r['Low'] - r['Close'].shift(1)).abs()\n",
    "        )\n",
    "    )).rolling(14, min_periods=1).mean()\n",
    "    \n",
    "    # Volume-based indicators\n",
    "    r['OBV'] = (r['Volume'] * ((r['Close'] - r['Close'].shift(1)).ge(0) * 2 - 1)).cumsum()\n",
    "    r['Volume_SMA_10'] = r['Volume'].rolling(10, min_periods=1).mean()\n",
    "    r['Volume_SMA_20'] = r['Volume'].rolling(20, min_periods=1).mean()\n",
    "    r['Volume_Ratio'] = r['Volume'] / r['Volume_SMA_20']\n",
    "    \n",
    "    # Price Channels\n",
    "    for period in [20, 50]:\n",
    "        r[f'Upper_Channel_{period}'] = r['High'].rolling(period, min_periods=1).max()\n",
    "        r[f'Lower_Channel_{period}'] = r['Low'].rolling(period, min_periods=1).min()\n",
    "        r[f'Channel_Position_{period}'] = (r['Close'] - r[f'Lower_Channel_{period}']) / (r[f'Upper_Channel_{period}'] - r[f'Lower_Channel_{period}'] + 1e-8)\n",
    "\n",
    "    # Add percentage returns\n",
    "    r['Returns'] = r['Close'].pct_change()\n",
    "    r['Log_Returns'] = np.log(r['Close'] / r['Close'].shift(1))\n",
    "    \n",
    "    return r\n",
    "\n",
    "def add_market_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    m = df.copy()\n",
    "    \n",
    "    # Group by date to calculate market-wide metrics\n",
    "    date_groups = m.groupby('Date')\n",
    "    \n",
    "    # Market-wide metrics\n",
    "    market_avg = date_groups['Close'].transform('mean')\n",
    "    market_std = date_groups['Close'].transform('std')\n",
    "    market_volume = date_groups['Volume'].transform('sum')\n",
    "    market_returns = market_avg.pct_change()\n",
    "    \n",
    "    # Relative strength metrics\n",
    "    m['Relative_Strength'] = m['Close'] / (market_avg + 1e-8)\n",
    "    m['Market_Volatility'] = date_groups['Returns'].transform('std')\n",
    "    m['Volume_Share'] = m['Volume'] / (market_volume + 1e-8)\n",
    "    m['Market_Momentum'] = market_returns.rolling(10).mean()\n",
    "    \n",
    "    # Calculate Beta (20-day rolling) - fixed version\n",
    "    for ticker in m['Ticker'].unique():\n",
    "        mask = m['Ticker'] == ticker\n",
    "        stock_returns = m.loc[mask, 'Close'].pct_change()\n",
    "        \n",
    "        # Calculate rolling covariance and variance\n",
    "        rolling_cov = stock_returns.rolling(20).cov(market_returns[mask])\n",
    "        rolling_var = market_returns[mask].rolling(20).var()\n",
    "        \n",
    "        # Calculate beta avoiding division by zero\n",
    "        m.loc[mask, 'Beta_20D'] = rolling_cov / (rolling_var + 1e-8)\n",
    "    \n",
    "    # Calculate Market Correlation (20-day rolling) - fixed version\n",
    "    for ticker in m['Ticker'].unique():\n",
    "        mask = m['Ticker'] == ticker\n",
    "        stock_returns = m.loc[mask, 'Close'].pct_change()\n",
    "        m.loc[mask, 'Market_Correlation'] = stock_returns.rolling(20).corr(market_returns[mask])\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Execute feature engineering\n",
    "if 'cleaned_data' in locals() and not cleaned_data.empty:\n",
    "    print(\"Starting feature engineering...\")\n",
    "    feature_data = []\n",
    "    \n",
    "    for t in tqdm(cleaned_data['Ticker'].unique(), desc=\"Engineering features\"):\n",
    "        try:\n",
    "            td = cleaned_data[cleaned_data['Ticker'] == t].copy()\n",
    "            if len(td) < 50:\n",
    "                print(f\"Skipping {t} - insufficient data points ({len(td)})\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Processing {t}...\")\n",
    "            td = calculate_technical_indicators(td)\n",
    "            feature_data.append(td)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {t}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not feature_data:\n",
    "        print(\"❌ No features generated for any stock\")\n",
    "    else:\n",
    "        print(\"Combining features...\")\n",
    "        engineered_data = pd.concat(feature_data, ignore_index=True)\n",
    "        print(\"Adding market features...\")\n",
    "        engineered_data = add_market_features(engineered_data)\n",
    "\n",
    "        # Handle missing values more robustly\n",
    "        critical = ['Close', 'Volume', 'Returns']\n",
    "        engineered_data = engineered_data.dropna(subset=[c for c in critical if c in engineered_data.columns])\n",
    "        \n",
    "        # Forward fill then backward fill within each stock\n",
    "        engineered_data = engineered_data.groupby('Ticker').apply(\n",
    "            lambda x: x.fillna(method='ffill').fillna(method='bfill')\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # Save engineered features\n",
    "        engineered_data.to_csv(f\"{CONFIG['data_dir']}/engineered_stock_data.csv\", index=False)\n",
    "        print(\"✅ Engineered and saved\", engineered_data.shape)\n",
    "        \n",
    "        # Print feature correlation with target\n",
    "        target = engineered_data[CONFIG['target_col']]\n",
    "        correlations = engineered_data.select_dtypes(include=[np.number]).corrwith(target)\n",
    "        print(\"\\nTop 10 features by correlation with target:\")\n",
    "        print(correlations.abs().sort_values(ascending=False).head(10))\n",
    "else:\n",
    "    print(\"❌ No cleaned data available for feature engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created datasets for 10 stocks\n"
     ]
    }
   ],
   "source": [
    "# 5. Stock-Specific Splits & Datasets\n",
    "\n",
    "def select_core_features(data: pd.DataFrame, max_features: int = 15) -> List[str]:\n",
    "    core = ['Close', 'Volume', 'Returns', 'High', 'Low']\n",
    "    candidates = [\n",
    "        'SMA_20','SMA_50','EMA_12','EMA_26','MACD','MACD_Signal','BB_Upper','BB_Lower',\n",
    "        'RSI_14','Stoch_K','Stoch_D','Momentum_10','ATR_14','Relative_Strength','Market_Volatility'\n",
    "    ]\n",
    "    available = [c for c in candidates if c in data.columns]\n",
    "    scores = {}\n",
    "    price_changes = data['Close'].pct_change().fillna(0)\n",
    "    for col in available:\n",
    "        try:\n",
    "            corr = abs(data[col].corr(price_changes))\n",
    "            if not np.isnan(corr):\n",
    "                scores[col] = corr\n",
    "        except Exception:\n",
    "            continue\n",
    "    top = [k for k, _ in sorted(scores.items(), key=lambda x: x[1], reverse=True)]\n",
    "    selected = core + top[: max(0, max_features - len(core))]\n",
    "    return selected\n",
    "\n",
    "\n",
    "def create_stock_specific_splits(data: pd.DataFrame, train_years=7, val_years=1.5, test_years=1.5):\n",
    "    splits = {}\n",
    "    for t in data['Ticker'].unique():\n",
    "        td = data[data['Ticker'] == t].sort_values('Date').copy()\n",
    "        if len(td) < 100:\n",
    "            continue\n",
    "        start = td['Date'].min()\n",
    "        train_end = start + pd.Timedelta(days=int(train_years * 365.25))\n",
    "        val_end = train_end + pd.Timedelta(days=int(val_years * 365.25))\n",
    "        train = td[td['Date'] <= train_end]\n",
    "        val = td[(td['Date'] > train_end) & (td['Date'] <= val_end)]\n",
    "        test = td[td['Date'] > val_end]\n",
    "        splits[t] = {\n",
    "            'train': train,\n",
    "            'val': val,\n",
    "            'test': test,\n",
    "            'train_dates': (train['Date'].min(), train['Date'].max()),\n",
    "            'val_dates': (val['Date'].min(), val['Date'].max()),\n",
    "            'test_dates': (test['Date'].min(), test['Date'].max()),\n",
    "        }\n",
    "    return splits\n",
    "\n",
    "class StockSpecificDataset(Dataset):\n",
    "    def __init__(self, data, feature_cols, target_col, seq_len, pred_len, scaler=None):\n",
    "        self.data = data.copy()\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.scaler = scaler or RobustScaler()\n",
    "        self.X, self.y = self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        seqs, tgts = [] , []\n",
    "        if len(self.data) < self.seq_len + self.pred_len:\n",
    "            return np.array([]), np.array([])\n",
    "        features = self.data[self.feature_cols].values\n",
    "        target = self.data[self.target_col].values\n",
    "        f_scaled = self.scaler.fit_transform(features)\n",
    "        t_scaled = self.scaler.fit_transform(target.reshape(-1,1)).flatten()\n",
    "        for i in range(len(f_scaled) - self.seq_len - self.pred_len + 1):\n",
    "            seq = f_scaled[i:i+self.seq_len]\n",
    "            tgt = t_scaled[i+self.seq_len:i+self.seq_len+self.pred_len]\n",
    "            if not np.isnan(seq).any() and not np.isnan(tgt).any():\n",
    "                seqs.append(seq); tgts.append(tgt)\n",
    "        return np.array(seqs, dtype=np.float32), np.array(tgts, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx]), torch.tensor(self.y[idx])\n",
    "\n",
    "\n",
    "def create_stock_specific_datasets(stock_splits, selected_features, target_col, seq_len, pred_len):\n",
    "    out = {}\n",
    "    for t, s in stock_splits.items():\n",
    "        train_ds = StockSpecificDataset(s['train'], selected_features, target_col, seq_len, pred_len)\n",
    "        if len(train_ds) == 0:\n",
    "            continue\n",
    "        val_ds = StockSpecificDataset(s['val'], selected_features, target_col, seq_len, pred_len, scaler=train_ds.scaler)\n",
    "        test_ds = StockSpecificDataset(s['test'], selected_features, target_col, seq_len, pred_len, scaler=train_ds.scaler)\n",
    "        out[t] = {\n",
    "            'train_dataset': train_ds,\n",
    "            'val_dataset': val_ds,\n",
    "            'test_dataset': test_ds,\n",
    "            'train_loader': DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0),\n",
    "            'val_loader': DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0),\n",
    "            'test_loader': DataLoader(test_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0),\n",
    "            'scaler': train_ds.scaler,\n",
    "            'feature_cols': selected_features,\n",
    "        }\n",
    "    return out\n",
    "\n",
    "if 'engineered_data' in locals() and not engineered_data.empty:\n",
    "    selected_features = select_core_features(engineered_data, max_features=15)\n",
    "    stock_splits = create_stock_specific_splits(engineered_data, CONFIG['train_years'], CONFIG['val_years'], CONFIG['test_years'])\n",
    "    stock_datasets = create_stock_specific_datasets(stock_splits, selected_features, CONFIG['target_col'], CONFIG['sequence_length'], CONFIG['forecast_horizon'])\n",
    "    print(f\"✅ Created datasets for {len(stock_datasets)} stocks\")\n",
    "else:\n",
    "    print(\"❌ No engineered data to split\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUGFIX SUMMARY: Hidden State Dimension Mismatch\n",
    "\n",
    "**Error Fixed:** `Expected hidden[0] size (2, 128, 512), got [4, 128, 256]`\n",
    "\n",
    "### Root Cause\n",
    "The ImprovedLSTM model had a critical mismatch between encoder and decoder hidden states:\n",
    "- **Encoder**: Uses **bidirectional=True** which produces hidden states with shape `(num_layers*2, batch, hidden_dim)`\n",
    "- **Decoder**: Was expecting hidden states with shape `(num_layers, batch, hidden_dim)` but received mismatched dimensions\n",
    "\n",
    "### Why This Happened\n",
    "When using a bidirectional LSTM:\n",
    "- Output hidden states have shape: `(num_layers * 2, batch_size, hidden_dim)`  \n",
    "- The 2x multiplier comes from forward and backward layers\n",
    "- The decoder LSTM must have the same hidden_size as the encoder's output dimension\n",
    "- Decoder's hidden_size must be: `hidden_dim * 2` (for bidirectional) to match encoder output\n",
    "\n",
    "### Changes Made\n",
    "1. **Fixed context extraction**: Now uses `enc_out[:, -1, :]` (last encoder output) instead of manipulated hidden states\n",
    "2. **Proper hidden state reshaping**: \n",
    "   - Reshape encoder hidden states from `(num_layers*2, batch, hidden_dim)` to `(num_layers, batch, hidden_dim*2)`\n",
    "   - This concatenates forward and backward hidden states properly\n",
    "3. **Updated decoder configuration**: Decoder's hidden_size is now set to `hidden_dim * (2 if bidirectional else 1)`\n",
    "4. **Updated output layers**: LayerNorm and projection layers now use correct `decoder_hidden_size`\n",
    "\n",
    "### Result\n",
    "Model now trains successfully with proper dimension alignment throughout the encoder-decoder architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. LSTM Model Development\n",
    "\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 128, num_layers: int = 2, dropout: float = 0.1, out_len: int = 5):\n",
    "        super().__init__()\n",
    "        self.out_len = out_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.decoder = nn.LSTM(input_size=1, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.proj = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, seq_len, input_dim]\n",
    "        enc_out, (h, c) = self.encoder(x)\n",
    "        # Teacher-forcing free decoder: start from last target context = last encoder output projected\n",
    "        batch = x.size(0)\n",
    "        dec_in = torch.zeros(batch, 1, 1, device=x.device)\n",
    "        outputs = []\n",
    "        h_t, c_t = h, c\n",
    "        for _ in range(self.out_len):\n",
    "            dec_out, (h_t, c_t) = self.decoder(dec_in, (h_t, c_t))\n",
    "            step = self.proj(dec_out)  # [B,1,1]\n",
    "            outputs.append(step)\n",
    "            dec_in = step.detach()  # autoregressive\n",
    "        return torch.cat(outputs, dim=1)  # [B, out_len, 1]\n",
    "\n",
    "\n",
    "class DirectionalSmoothLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.3, gamma=0.2):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.alpha, self.beta, self.gamma = alpha, beta, gamma\n",
    "    def forward(self, pred, target):\n",
    "        mse = self.mse(pred, target)\n",
    "        if pred.shape[1] > 1:\n",
    "            pdiff = pred[:,1:] - pred[:,:-1]\n",
    "            tdiff = target[:,1:] - target[:,:-1]\n",
    "            dir_loss = torch.mean((torch.sign(pdiff) - torch.sign(tdiff))**2)\n",
    "            smooth = torch.mean(torch.abs(pdiff))\n",
    "        else:\n",
    "            dir_loss = pred.new_tensor(0.0)\n",
    "            smooth = pred.new_tensor(0.0)\n",
    "        return self.alpha*mse + self.beta*dir_loss + self.gamma*smooth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training model for BHARTIARTL.NS\n",
      "============================================================\n",
      "📊 Training BHARTIARTL.NS for 100 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124cc3721b6844308879bab129fd92f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error training BHARTIARTL.NS: Expected hidden[0] size (2, 128, 512), got [4, 128, 256]\n",
      "\n",
      "============================================================\n",
      "Training model for HDFCBANK.NS\n",
      "============================================================\n",
      "📊 Training HDFCBANK.NS for 100 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78611e6fad4544378b92f0465eb03526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error training HDFCBANK.NS: Expected hidden[0] size (2, 128, 512), got [4, 128, 256]\n",
      "\n",
      "============================================================\n",
      "Training model for HINDUNILVR.NS\n",
      "============================================================\n",
      "📊 Training HINDUNILVR.NS for 100 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ac771c0cb244aa97b72b3f00ad2e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error training HINDUNILVR.NS: Expected hidden[0] size (2, 128, 512), got [4, 128, 256]\n",
      "\n",
      "============================================================\n",
      "Training model for ICICIBANK.NS\n",
      "============================================================\n",
      "📊 Training ICICIBANK.NS for 100 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70446d44990741b3bb9ebb44f3a1e10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error training ICICIBANK.NS: Expected hidden[0] size (2, 128, 512), got [4, 128, 256]\n",
      "\n",
      "============================================================\n",
      "Training model for INFY.NS\n",
      "============================================================\n",
      "📊 Training INFY.NS for 100 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61655077193d46e88b5e60d1cdb8c5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error training INFY.NS: Expected hidden[0] size (2, 128, 512), got [4, 128, 256]\n",
      "\n",
      "============================================================\n",
      "Training model for ITC.NS\n",
      "============================================================\n",
      "📊 Training ITC.NS for 100 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50a667db88e4d939029b2bc098aea3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error training ITC.NS: Expected hidden[0] size (2, 128, 512), got [4, 128, 256]\n",
      "\n",
      "============================================================\n",
      "Training model for KOTAKBANK.NS\n",
      "============================================================\n",
      "📊 Training KOTAKBANK.NS for 100 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b8a55957f74fa1ae479ba613968855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error training KOTAKBANK.NS: Expected hidden[0] size (2, 128, 512), got [4, 128, 256]\n",
      "\n",
      "============================================================\n",
      "Training model for LT.NS\n",
      "============================================================\n",
      "📊 Training LT.NS for 100 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9cf0104d24d4ce3b37bbcae93a8dde0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error training LT.NS: Expected hidden[0] size (2, 128, 512), got [4, 128, 256]\n",
      "\n",
      "============================================================\n",
      "Training model for RELIANCE.NS\n",
      "============================================================\n",
      "📊 Training RELIANCE.NS for 100 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98debf71993f41a7bc5b5ffe35b3833e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error training RELIANCE.NS: Expected hidden[0] size (2, 128, 512), got [4, 128, 256]\n",
      "\n",
      "============================================================\n",
      "Training model for TCS.NS\n",
      "============================================================\n",
      "📊 Training TCS.NS for 100 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94721ed8ff9a431c917b277abf08b4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error training TCS.NS: Expected hidden[0] size (2, 128, 512), got [4, 128, 256]\n",
      "\n",
      "✅ Stock-wise training completed\n",
      "\n",
      "Training Summary:\n",
      "========================================\n",
      "Average best validation loss: nan\n",
      "Average number of epochs: nan\n"
     ]
    }
   ],
   "source": [
    "# 7. Model Training (Per-Stock)\n",
    "\n",
    "def get_lr_scheduler(optimizer, config):\n",
    "    if config['lr_scheduler'] == 'cosine':\n",
    "        return optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=config['warmup_epochs'],\n",
    "            T_mult=2,\n",
    "            eta_min=config['min_lr']\n",
    "        )\n",
    "    elif config['lr_scheduler'] == 'reduce_on_plateau':\n",
    "        return optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            verbose=True,\n",
    "            min_lr=config['min_lr']\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler type: {config['lr_scheduler']}\")\n",
    "\n",
    "def train_stock_model(ticker: str, datasets: Dict, device: str):\n",
    "    train_loader = datasets['train_loader']\n",
    "    val_loader = datasets['val_loader']\n",
    "    feature_cols = datasets['feature_cols']\n",
    "    \n",
    "    # Initialize model with updated architecture\n",
    "    model = ImprovedLSTM(\n",
    "        input_dim=len(feature_cols),\n",
    "        hidden_dim=CONFIG['hidden_dim'],\n",
    "        num_layers=CONFIG['num_layers'],\n",
    "        dropout=CONFIG['dropout_rate'],\n",
    "        bidirectional=CONFIG['bidirectional'],\n",
    "        attention=CONFIG['attention'],\n",
    "        out_len=CONFIG['forecast_horizon']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize loss and optimizer\n",
    "    criterion = EnhancedLoss(\n",
    "        mse_weight=CONFIG['mse_weight'],\n",
    "        directional_weight=CONFIG['directional_weight'],\n",
    "        smoothness_weight=CONFIG['smoothness_weight']\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Initialize learning rate scheduler\n",
    "    scheduler = get_lr_scheduler(optimizer, CONFIG)\n",
    "    \n",
    "    # Training state tracking\n",
    "    best_val = float('inf')\n",
    "    best_epoch = 0\n",
    "    patience_cnt = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    print(f\"📊 Training {ticker} for {CONFIG['epochs']} epochs...\")\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{CONFIG[\"epochs\"]}')\n",
    "        \n",
    "        for xb, yb in progress:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            \n",
    "            # Ensure compatible shapes\n",
    "            if out.dim() > yb.dim():\n",
    "                out = out.squeeze(-1)\n",
    "            if yb.dim() > out.dim():\n",
    "                yb = yb.squeeze(-1)\n",
    "                \n",
    "            # Calculate loss and backpropagate\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update progress\n",
    "            tr_loss += loss.item()\n",
    "            progress.set_postfix({'loss': f'{loss.item():.5f}'})\n",
    "            \n",
    "        tr_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        va_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                if out.dim() > yb.dim():\n",
    "                    out = out.squeeze(-1)\n",
    "                if yb.dim() > out.dim():\n",
    "                    yb = yb.squeeze(-1)\n",
    "                loss = criterion(out, yb)\n",
    "                va_loss += loss.item()\n",
    "        va_loss /= len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        if CONFIG['lr_scheduler'] == 'reduce_on_plateau':\n",
    "            scheduler.step(va_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Record losses\n",
    "        train_losses.append(tr_loss)\n",
    "        val_losses.append(va_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 5 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch:3d} | Train: {tr_loss:.5f} | Val: {va_loss:.5f} | LR: {current_lr:.2e}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            best_epoch = epoch\n",
    "            patience_cnt = 0\n",
    "            \n",
    "            # Save model with additional metadata\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'val_loss': va_loss,\n",
    "                'feature_cols': feature_cols,\n",
    "                'config': CONFIG,\n",
    "                'best_epoch': best_epoch\n",
    "            }, f\"{CONFIG['model_dir']}/best_model_{ticker}.pth\")\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            \n",
    "        # Early stopping check\n",
    "        if patience_cnt >= CONFIG['patience']:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best epoch was {best_epoch}\")\n",
    "            break\n",
    "    \n",
    "    return model, train_losses, val_losses, best_epoch\n",
    "\n",
    "# Train models for each stock\n",
    "if 'stock_datasets' in locals():\n",
    "    stock_models = {}\n",
    "    training_results = {}\n",
    "    \n",
    "    for t, ds in stock_datasets.items():\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Training model for {t}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            model, tr_losses, va_losses, best_epoch = train_stock_model(t, ds, CONFIG['device'])\n",
    "            stock_models[t] = model\n",
    "            training_results[t] = {\n",
    "                'train_losses': tr_losses,\n",
    "                'val_losses': va_losses,\n",
    "                'final_train_loss': tr_losses[-1] if tr_losses else None,\n",
    "                'final_val_loss': va_losses[-1] if va_losses else None,\n",
    "                'best_val_loss': min(va_losses) if va_losses else None,\n",
    "                'best_epoch': best_epoch,\n",
    "                'total_epochs': len(tr_losses)\n",
    "            }\n",
    "            \n",
    "            # Print summary statistics\n",
    "            print(f\"\\nResults for {t}:\")\n",
    "            print(f\"Best epoch: {best_epoch}\")\n",
    "            print(f\"Best validation loss: {min(va_losses):.5f}\")\n",
    "            print(f\"Final training loss: {tr_losses[-1]:.5f}\")\n",
    "            print(f\"Total epochs trained: {len(tr_losses)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error training {t}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    print(\"\\n✅ Stock-wise training completed\")\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(\"\\nTraining Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    avg_best_val = np.mean([r['best_val_loss'] for r in training_results.values()])\n",
    "    avg_epochs = np.mean([r['total_epochs'] for r in training_results.values()])\n",
    "    print(f\"Average best validation loss: {avg_best_val:.5f}\")\n",
    "    print(f\"Average number of epochs: {avg_epochs:.1f}\")\n",
    "else:\n",
    "    print(\"❌ No datasets to train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation completed\n"
     ]
    }
   ],
   "source": [
    "# 8. Evaluation & Visualizations\n",
    "\n",
    "def evaluate_stock_model(ticker: str, model: nn.Module, datasets: Dict, device: str):\n",
    "    test_loader = datasets['test_loader']\n",
    "    scaler = datasets['target_scaler'] if 'target_scaler' in datasets else datasets['scaler']\n",
    "\n",
    "    model.eval()\n",
    "    preds, tgts = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            if out.dim() == 3 and out.shape[-1] == 1:\n",
    "                out = out.squeeze(-1)\n",
    "            if yb.dim() == 3 and yb.shape[-1] == 1:\n",
    "                yb = yb.squeeze(-1)\n",
    "            # Use first-step horizon for alignment\n",
    "            preds.extend(out[:, 0].cpu().numpy().flatten())\n",
    "            tgts.extend(yb[:, 0].cpu().numpy().flatten())\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    tgts = np.array(tgts)\n",
    "    n = min(len(preds), len(tgts))\n",
    "    preds, tgts = preds[:n], tgts[:n]\n",
    "\n",
    "    pred_inv = scaler.inverse_transform(preds.reshape(-1,1)).flatten()\n",
    "    tgt_inv = scaler.inverse_transform(tgts.reshape(-1,1)).flatten()\n",
    "\n",
    "    mse = mean_squared_error(tgt_inv, pred_inv)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(tgt_inv, pred_inv)\n",
    "    mape = np.mean(np.abs((tgt_inv - pred_inv) / (tgt_inv + 1e-8))) * 100\n",
    "    r2 = r2_score(tgt_inv, pred_inv)\n",
    "\n",
    "    if len(tgt_inv) > 1:\n",
    "        dir_acc = np.mean(np.sign(np.diff(tgt_inv)) == np.sign(np.diff(pred_inv))) * 100\n",
    "    else:\n",
    "        dir_acc = np.nan\n",
    "\n",
    "    return {\n",
    "        'predictions': pred_inv,\n",
    "        'targets': tgt_inv,\n",
    "        'metrics': {'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2, 'Directional_Accuracy': dir_acc}\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_stock_results(ticker: str, results: Dict, stock_splits: Dict):\n",
    "    import re\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'Stock-Specific Analysis: {ticker}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    splits = stock_splits[ticker]\n",
    "    test_data = splits['test']\n",
    "\n",
    "    preds = results['predictions']\n",
    "    tgts = results['targets']\n",
    "\n",
    "    # 1) Time series\n",
    "    ax1 = axes[0,0]\n",
    "    ax1.plot(test_data['Date'], test_data['Close'], label='Actual Test Data', color='blue', linewidth=2, alpha=0.8)\n",
    "    if len(preds) > 0 and len(tgts) > 0:\n",
    "        start_idx = CONFIG['sequence_length']\n",
    "        end_idx = min(start_idx + len(preds), len(test_data))\n",
    "        pred_dates = test_data['Date'].iloc[start_idx:end_idx]\n",
    "        ax1.plot(pred_dates, preds[:len(pred_dates)], label='Predictions (1-step ahead)', color='red', linewidth=2, alpha=0.8)\n",
    "    ax1.set_title(f'{ticker} - Actual vs Predicted Prices')\n",
    "    ax1.set_xlabel('Date'); ax1.set_ylabel('Price'); ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2) Scatter\n",
    "    ax2 = axes[0,1]\n",
    "    if len(preds) > 0 and len(tgts) > 0:\n",
    "        ax2.scatter(tgts, preds, alpha=0.6, color='blue', s=30)\n",
    "        mn, mx = min(tgts.min(), preds.min()), max(tgts.max(), preds.max())\n",
    "        ax2.plot([mn, mx], [mn, mx], 'r--', lw=2)\n",
    "        ax2.text(0.05, 0.95, f\"R² = {results['metrics']['R2']:.4f}\", transform=ax2.transAxes,\n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        ax2.set_xlabel('Actual Price'); ax2.set_ylabel('Predicted Price'); ax2.set_title(f'{ticker} - Predictions vs Actual'); ax2.legend()\n",
    "    else:\n",
    "        ax2.text(0.5,0.5,'No prediction data', ha='center', va='center', transform=ax2.transAxes)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3) Residuals\n",
    "    ax3 = axes[1,0]\n",
    "    if len(preds) > 0 and len(tgts) > 0:\n",
    "        res = tgts - preds\n",
    "        ax3.scatter(preds, res, alpha=0.6, color='green', s=30)\n",
    "        ax3.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "        ax3.text(0.05, 0.95, f\"Mean: {np.mean(res):.2f}\\nStd: {np.std(res):.2f}\", transform=ax3.transAxes,\n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        ax3.set_xlabel('Predicted Price'); ax3.set_ylabel('Residuals (Actual - Predicted)'); ax3.set_title(f'{ticker} - Residuals')\n",
    "    else:\n",
    "        ax3.text(0.5,0.5,'No residuals', ha='center', va='center', transform=ax3.transAxes)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4) Metrics bar\n",
    "    ax4 = axes[1,1]\n",
    "    m = results['metrics']\n",
    "    names = ['RMSE','MAE','MAPE(%)','R²','Dir_Acc(%)']\n",
    "    vals = [m['RMSE'], m['MAE'], m['MAPE'], max(0, m['R2'])*100, m['Directional_Accuracy'] if not np.isnan(m['Directional_Accuracy']) else 0]\n",
    "    bars = ax4.bar(names, vals, color=['red','orange','yellow','green','blue'], alpha=0.7)\n",
    "    ax4.set_title(f'{ticker} - Performance Metrics'); ax4.set_ylabel('Metric Value'); ax4.tick_params(axis='x', rotation=45)\n",
    "    for b, v in zip(bars, vals):\n",
    "        ax4.text(b.get_x()+b.get_width()/2., b.get_height()+max(vals)*0.01, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    safe_ticker = re.sub(r'[^A-Za-z0-9_-]+', '_', ticker)\n",
    "    plt.savefig(f\"{CONFIG['results_dir']}/{safe_ticker}_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "if 'stock_models' in locals() and 'stock_datasets' in locals():\n",
    "    stock_results = {}\n",
    "    all_metrics = {}\n",
    "    for t, model in stock_models.items():\n",
    "        print(f\"\\n📊 Evaluating {t}...\")\n",
    "        try:\n",
    "            res = evaluate_stock_model(t, model, stock_datasets[t], CONFIG['device'])\n",
    "            stock_results[t] = res\n",
    "            all_metrics[t] = res['metrics']\n",
    "            plot_stock_results(t, res, stock_splits)\n",
    "            print(f\"✅ {t} evaluation done | RMSE: {res['metrics']['RMSE']:.4f} | MAE: {res['metrics']['MAE']:.4f} | MAPE: {res['metrics']['MAPE']:.2f}% | R²: {res['metrics']['R2']:.4f} | DirAcc: {res['metrics']['Directional_Accuracy']:.2f}%\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error evaluating {t}: {e}\")\n",
    "            continue\n",
    "    print(\"\\n✅ Evaluation completed\")\n",
    "else:\n",
    "    print(\"❌ Missing models/datasets for evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 LSTM STOCK-SPECIFIC RESULTS SUMMARY\n",
      "================================================================================\n",
      "Total stocks evaluated: 0\n",
      "Sequence length: 180 | Horizon: 5\n",
      "Stock           RMSE       MAE        MAPE(%)    R²         DirAcc(%)   \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m summary\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mstock_results\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mall_metrics\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     results_summary = \u001b[43mcreate_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_metrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Analysis complete\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mcreate_summary\u001b[39m\u001b[34m(stock_results, all_metrics)\u001b[39m\n\u001b[32m     21\u001b[39m summary = {\n\u001b[32m     22\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtotal_stocks\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(stock_results),\n\u001b[32m     23\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msequence_length\u001b[39m\u001b[33m'\u001b[39m: CONFIG[\u001b[33m'\u001b[39m\u001b[33msequence_length\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     }\n\u001b[32m     33\u001b[39m }\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mresults_dir\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/project_summary.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[43mjson\u001b[49m.dump(\n\u001b[32m     36\u001b[39m         summary,\n\u001b[32m     37\u001b[39m         f,\n\u001b[32m     38\u001b[39m         indent=\u001b[32m2\u001b[39m,\n\u001b[32m     39\u001b[39m         default=\u001b[38;5;28;01mlambda\u001b[39;00m o: \u001b[38;5;28mfloat\u001b[39m(o) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, (np.floating, np.integer)) \u001b[38;5;28;01melse\u001b[39;00m (o.tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, np.ndarray) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(o))\n\u001b[32m     40\u001b[39m     )\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m💾 Saved summary to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mresults_dir\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/project_summary.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m summary\n",
      "\u001b[31mNameError\u001b[39m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# 9. Results Summary\n",
    "\n",
    "def create_summary(stock_results: Dict, all_metrics: Dict):\n",
    "    print(\"🎯 LSTM STOCK-SPECIFIC RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total stocks evaluated: {len(stock_results)}\")\n",
    "    print(f\"Sequence length: {CONFIG['sequence_length']} | Horizon: {CONFIG['forecast_horizon']}\")\n",
    "\n",
    "    # Table\n",
    "    print(f\"{'Stock':<15} {'RMSE':<10} {'MAE':<10} {'MAPE(%)':<10} {'R²':<10} {'DirAcc(%)':<12}\")\n",
    "    print(\"-\"*80)\n",
    "    for t, m in all_metrics.items():\n",
    "        print(f\"{t:<15} {m['RMSE']:<10.4f} {m['MAE']:<10.4f} {m['MAPE']:<10.2f} {m['R2']:<10.4f} {m['Directional_Accuracy']:<12.2f}\")\n",
    "\n",
    "    rmse_vals = [m['RMSE'] for m in all_metrics.values()]\n",
    "    mae_vals = [m['MAE'] for m in all_metrics.values()]\n",
    "    mape_vals = [m['MAPE'] for m in all_metrics.values()]\n",
    "    r2_vals = [m['R2'] for m in all_metrics.values()]\n",
    "    dir_vals = [m['Directional_Accuracy'] for m in all_metrics.values() if not np.isnan(m['Directional_Accuracy'])]\n",
    "\n",
    "    summary = {\n",
    "        'total_stocks': len(stock_results),\n",
    "        'sequence_length': CONFIG['sequence_length'],\n",
    "        'prediction_horizon': CONFIG['forecast_horizon'],\n",
    "        'individual_metrics': all_metrics,\n",
    "        'aggregate_stats': {\n",
    "            'mean_rmse': float(np.mean(rmse_vals)) if rmse_vals else None,\n",
    "            'mean_mae': float(np.mean(mae_vals)) if mae_vals else None,\n",
    "            'mean_mape': float(np.mean(mape_vals)) if mape_vals else None,\n",
    "            'mean_r2': float(np.mean(r2_vals)) if r2_vals else None,\n",
    "            'mean_dir_acc': float(np.mean(dir_vals)) if dir_vals else None,\n",
    "        }\n",
    "    }\n",
    "    with open(f\"{CONFIG['results_dir']}/project_summary.json\", 'w') as f:\n",
    "        json.dump(\n",
    "            summary,\n",
    "            f,\n",
    "            indent=2,\n",
    "            default=lambda o: float(o) if isinstance(o, (np.floating, np.integer)) else (o.tolist() if isinstance(o, np.ndarray) else str(o))\n",
    "        )\n",
    "    print(f\"💾 Saved summary to {CONFIG['results_dir']}/project_summary.json\")\n",
    "    return summary\n",
    "\n",
    "if 'stock_results' in locals() and 'all_metrics' in locals():\n",
    "    results_summary = create_summary(stock_results, all_metrics)\n",
    "    print(\"✅ Analysis complete\")\n",
    "else:\n",
    "    print(\"❌ No results available for summary\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
